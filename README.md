# Dynamic-live Neural Networks
Dynamic-Live neural networks are type of RNN. However they differ in an important way, the connections between each neuron is learnable. When a neuron is created it is not connected to anything. After every neuron we want to have in total is created we call a function and pass it a list of neurons. We do this for each neuron in our network. That list is every neuron that the a particular neuron is allowed to get input from. The network learns how to form good connections between every neuron.

## How does it work?
The main feature of this network is not yet finished, we are still in the phase of creating basic functionalities such as the training functions. A lot of these things are harder to implement because of the nature of the network. Below is what training the network looks like as of now.
### Input propogation
This is how basic predictions are made in this network. Just about any other neural network implements this in some for or another. This is a single function in our library.
#### Priming
This makes the neuron go and get it's inputs from each connected neuron. We can't get the inputs when we fire the neurons because doing so would cause some problems with the data that the neurons get, depending on the order that they are fired.
#### Firing
Each neuron in a layer is fired. This takes the dot product of the weights stored in the neuron and the inputs we got in the priming step. Then an activation function of the users design (custom activations supported) and the output of the neuron is stored in a variable. This is repeated for every layer in order. It's worth noting that layering is only used to tell the system what order to fire groups of neuron and has nothing to do with what neurons can connect where. After every neuron has fired the output of every neuron in the output layer is gathered into an array and returned.
### Backpropogation
Traditionaly we calculate the loss and calculate changes for each layer. We do things differently durring the priming and firing steps memory data is collected in the background, up to a certain amount set by the user. This data will not influence predictions made, it is instead used to get context of for training because in complex networks a neurons actions might not display until 5, 50 or even 500 cycles later. When we train the output over all output we are referencing is fed into each neurons training function as a parameter. The neuron looks at the contextual outputs index and then looks at the same index of another memroy bank for some internal values. These values are used in the calculation of the loss (custom loss functions are in the works) then we look at the derivative of the activation function and calculate gradients to update the weights with. After a neuron has made these changes it looks at every neuron it gets input from it finds that neurons ouptut it received and passes the signal up the network. This happens for every neuron which, for really large networks, can be computationaly expensive. This all seems like a really complex way to do backpropogation, but it (theoreticaly) allows for some extremely powerful training in terms of memory.
### Connection training
While I would love to say that this function is finished, it is not. Right now we are building networks by hand and they can't train the connections. Reasearch is still being done on how this will work. Keep in mind that this is a new concept that, as far as I can find, has not been aproached in this way. If you have concepts then please open an issue and explain your concept.
